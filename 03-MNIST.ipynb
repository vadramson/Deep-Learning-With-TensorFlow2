{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Deep Neural Network for MNIST Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset provides 70,000 images (28x28 pixels) of handwritten digits (1 digit per image).\n",
    "\n",
    "The goal is to write an algorithm that detects which digit is written. Since there are only \n",
    "\n",
    "10 digits (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), this is a classification problem with 10 classes.\n",
    "\n",
    "The goal hers is to build a neural network with 2 hidden layers.\n",
    "\n",
    "Each image consists of 28 * 28 = 784 pixels and each pixel will be an input for the neural network.\n",
    "\n",
    "Each pixel corresponds to to the intensity of the colour from 0 to 255 (Black to White).\n",
    "\n",
    "**The strategy will be:**\n",
    "\n",
    "    1- Prepare and preprocesse the data (Create Training, Validation and Test Datasets).\n",
    "    2- Outline a model and choose a validation function.\n",
    "    3- Set appropriate advanced optimizers and loss functions\n",
    "    4- Train the Model built (Use backpropagation to optimze each Epoch)\n",
    "    5- Test the accuracy of the model\n",
    "  \n",
    "  \n",
    "Le jeu de données fournit 70 000 images (28x28 pixels) de chiffres écrits à la main (1 chiffre par image).\n",
    "\n",
    "L'objectif est d'écrire un algorithme qui détecte quel chiffre est écrit. Comme il n'y a que\n",
    "\n",
    "10 chiffres (0, 1, 2, 3, 4, 5, 6, 7, 8, 9), il s'agit d'un problème de classification avec 10 classes.\n",
    "\n",
    "Son objectif est de construire un réseau neuronal avec 2 couches cachées.\n",
    "\n",
    "Chaque image est composée de 28 * 28 = 784 pixels et chaque pixel sera une entrée pour le réseau de neurones.\n",
    "\n",
    "Chaque pixel correspond à l'intensité de la couleur de 0 à 255 (Noir à Blanc).\n",
    "\n",
    "**La stratégie sera :**\n",
    "\n",
    "    1- Préparer et prétraiter les données (créer des jeux de données d'entraînement, de validation et de test).\n",
    "    2- Esquisser un modèle et choisir une fonction de validation.\n",
    "    3- Définir les optimisateurs avancés et les fonctions de perte appropriés.\n",
    "    4- Faites apprendre le modèle (utilisez la rétropropagation pour optimiser chaque époque).\n",
    "    5- Testez la précision du modèle\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabling eager execution\n",
      "INFO:tensorflow:Enabling v2 tensorshape\n",
      "INFO:tensorflow:Enabling resource variables\n",
      "INFO:tensorflow:Enabling tensor equality\n",
      "INFO:tensorflow:Enabling control flow v2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as_supervised=True will load the dataset in a 2-tuple structure (input, target) else it will load the data into a dictionary\n",
    "# with_info=True provide a tuple containing information about the version, features, number of samples\n",
    "# The information about the dataset will be stored in mnist_info\n",
    "\n",
    "mnist_dataset, mnist_info = tfds.load(name=\"mnist\", with_info=True, as_supervised=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spliting data into Train, Validation and Test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Training and Test data\n",
    "mnist_train = mnist_dataset[\"train\"] # Etract the training data\n",
    "mnist_test = mnist_dataset[\"test\"] # Etract the test data\n",
    "\n",
    "\n",
    "# Get the number of validation samples as a percentage of Training data to be extracted from Training data\n",
    "num_validation_samples = 0.1 * mnist_info.splits[\"train\"].num_examples # Get the number of 10% of training data\n",
    "num_validation_samples = tf.cast(num_validation_samples, tf.int64) # Make sure number is an integer\n",
    "\n",
    "\n",
    "# Get the number of test samples\n",
    "num_test_samples = mnist_info.splits[\"test\"].num_examples # Get number of test data\n",
    "num_test_samples = tf.cast(num_test_samples, tf.int64) # Make sure the number is an integer\n",
    "\n",
    "# Scaling the data to make it more numerical stable(e.g. inputs between 0 and 1)\n",
    "def scale_inputs(image, label): # takes an image and label\n",
    "    image = tf.cast(image, tf.float32) # Make sure all image values are floats\n",
    "    # Since all the MNIST images contain values of 0 to 255, dividing images by 255 will give values between 0 and 1\n",
    "    image /= 255.0 # the \".0\" enforces the results to be of type float\n",
    "    return image, label\n",
    "\n",
    "# Scaling the Training and validation data\n",
    "scaled_trained_and_validation_data = mnist_train.map(scale_inputs)\n",
    "test_data = mnist_test.map(scale_inputs) # Scales test data to same scale as training and validstion dsts\n",
    "\n",
    "\n",
    "# Shuffling data to have a more random spread of data to better optimize batching\n",
    "\n",
    "# Takes 10000 samples shuffle them and take the next 10000 till whole dataset is shuffled. \n",
    "# This technique optimizes the the use of memory resources as all the dataset can't be feed \n",
    "# into the computer's memory for \n",
    "\n",
    "BUFFER_SIZE = 10000 \n",
    "shuffled_scaled_trained_and_validation_data = scaled_trained_and_validation_data.shuffle(BUFFER_SIZE)\n",
    "\n",
    "# Extracting the Validation data from the neewly shuffled_scaled_trained_and_validation_data\n",
    "validation_data = shuffled_scaled_trained_and_validation_data.take(num_validation_samples)\n",
    "\n",
    "# Extracting the training data from neewly shuffled_scaled_trained_and_validation_data but skipping the validation_data\n",
    "training_data = shuffled_scaled_trained_and_validation_data.skip(num_validation_samples)\n",
    "\n",
    "# Using mini-batch Gradient descent to train the model as it optimises speed and sample size\n",
    "BATCH_SIZE = 100\n",
    "training_data = training_data.batch(BATCH_SIZE) # Adds a new column to the tensor that indicates to the model how many samples it should take for each batch\n",
    "validation_data = validation_data.batch(num_validation_samples)\n",
    "test_data = test_data.batch(num_test_samples)\n",
    "\n",
    "validation_inputs, validation_targets = next(iter(validation_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlining the Model\n",
    "\n",
    "Deep learning Algorithms are mostly about building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "output_size = 10\n",
    "# Use same hidden layer size for both hidden layers. Not a necessity.\n",
    "# hidden_layer_size = 50\n",
    "hidden_layer_size = 100\n",
    "\n",
    "# Defining how the model will look like\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    # the first layer (the input layer)\n",
    "    # each observation is 28x28x1 pixels, therefore it is a tensor of rank 3\n",
    "    # since we don't know CNNs yet, we don't know how to feed such input into our net, so we must flatten the images\n",
    "    # there is a convenient method 'Flatten' that simply takes our 28x28x1 tensor and orders it into a (None,) \n",
    "    # or (28x28x1,) = (784,) vector\n",
    "    # this allows us to actually create a feed forward neural network\n",
    "    \n",
    "    tf.keras.layers.Flatten(input_shape=(28,28,1)), # input layer\n",
    "    \n",
    "    # tf.keras.layers.Dense is basically implementing: output = activation(dot(input, weight) + bias)\n",
    "    # it takes several arguments, but the most important ones for us are the hidden_layer_size and the activation function\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 1st hidden layer\n",
    "    tf.keras.layers.Dense(hidden_layer_size, activation='relu'), # 2nd hidden layer\n",
    "    \n",
    "    # the final layer is no different, we just make sure to activate it with softmax\n",
    "    tf.keras.layers.Dense(output_size, activation='softmax') # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choose the optimizer and the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the optimizer to be used based on the problem, \n",
    "# the loss function, based on the type of encoding  \n",
    "# and the metrics to be obtained at each iteration\n",
    "\n",
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model\n",
    "\n",
    "Training the just built model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**During Training for each epoch:**\n",
    "\n",
    "    1- At the begining for each epoch, the training loss will be set to 0\n",
    "    2- The Algorithm will iterate over a preset number of batches from the training_data\n",
    "    3- The Weights and Baises will be updated as many times as there are batches\n",
    "    4- The values of the loss function will be displayed to show how the training is going\n",
    "    5- The Training accuracy will also be displayed\n",
    "    6- At the end of each epoch, the algorithm will forward propagate the whole validation dataset\n",
    "    7- The above steps will be repeated as many times as there are epochs and at the end of the last epoch, the training will be over.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "540/540 - 6s - loss: 0.0193 - accuracy: 0.9940 - val_loss: 0.0196 - val_accuracy: 0.9942\n",
      "Epoch 2/6\n",
      "540/540 - 5s - loss: 0.0182 - accuracy: 0.9939 - val_loss: 0.0258 - val_accuracy: 0.9918\n",
      "Epoch 3/6\n",
      "540/540 - 5s - loss: 0.0133 - accuracy: 0.9961 - val_loss: 0.0215 - val_accuracy: 0.9937\n",
      "Epoch 4/6\n",
      "540/540 - 5s - loss: 0.0126 - accuracy: 0.9960 - val_loss: 0.0131 - val_accuracy: 0.9958\n",
      "Epoch 5/6\n",
      "540/540 - 5s - loss: 0.0138 - accuracy: 0.9952 - val_loss: 0.0177 - val_accuracy: 0.9937\n",
      "Epoch 6/6\n",
      "540/540 - 5s - loss: 0.0109 - accuracy: 0.9967 - val_loss: 0.0117 - val_accuracy: 0.9955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x19eb4a6ed00>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NUM_EPOCHS = 5 # Number of epochs for which the Model should be trained\n",
    "NUM_EPOCHS = 6\n",
    "\n",
    "# Fit the model, specifying the\n",
    "# training data\n",
    "# the total number of epochs\n",
    "# and the validation data created in the format: (inputs,targets)\n",
    "\n",
    "model.fit(training_data, epochs=NUM_EPOCHS, validation_data = (validation_inputs, validation_targets), verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing the hidden_layer_size from 50 to hidden_layer_size = 100 increased the overall model accuracy(val_accuracy)  from 97% to 98% ( 0.9808).\n",
    "\n",
    "Changing NUM_EPOCHS from 5 to 6 100 increased the overall model accuracy(val_accuracy)  from 98% to 99% (0.9955)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training** \n",
    "\n",
    "**Validation**\n",
    "\n",
    "**Testing**\n",
    "\n",
    "During the training, overfitting was prevented by validating the model on the validation_data.\n",
    "After the first first training, each modification of the Hyperparameters, actually overfitted the validation dataset\n",
    "\n",
    "After training on the training data and validating on the validation data, the final prediction power of the model is gotten by running it on the test dataset that the algorithm has NEVER seen before.\n",
    "\n",
    "It is very important to realize that fiddling with the hyperparameters overfits the validation dataset.\n",
    "\n",
    "The test is the absolute final instance, hence testing should not be done before the model has completely been adjusted.\n",
    "\n",
    "If you adjust your model after testing, you will start overfitting the test dataset, which will defeat its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step - loss: 0.0873 - accuracy: 0.9803\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.09. Test accuracy: 98.03%\n"
     ]
    }
   ],
   "source": [
    "# We can apply some nice formatting if we want to\n",
    "print('Test loss: {0:.2f}. Test accuracy: {1:.2f}%'.format(test_loss, test_accuracy*100.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the initial model and hyperparameters given in this notebook, the final test accuracy should be roughly around 97%.\n",
    "\n",
    "Each time the code is rerun, there's a different accuracy as the batches are shuffled, the weights are initialized in a different way, etc.\n",
    "\n",
    "Finally, a suboptimal solution has intentionally been reached, so model deployment can actually take place here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3-TF2.0",
   "language": "python",
   "name": "py3-tf2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
